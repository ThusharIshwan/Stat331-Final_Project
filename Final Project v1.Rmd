---
title: "STAT 331 Final Project"
author: Deep Antala, Thushar Ishwanthlal, Vanessa Li, Harry Qu 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary
The objective of the report is to investigate the extent to which removing outliers from a dataset improves prediction accuracy. This is done through fitting and validating models with a holdout sample to determine prediction error. Then, outliers are removed from the dataset. Finally, the models are fit using the reduced dataset and validated once again to determine their new prediction error. The analysis finishes with a simulation that performs this test on a large number of linear models, producing a histogram of the changes in prediction error as a result of removing outliers. 

The report concludes that

## Objective

To investigate the extent to which removing outliers from a dataset improves prediction accuracy. 


\pagebreak
## Exploratory Data Analysis
This section will report only relevant final numbers and figures resulting from the exploratory data analysis; all R code and methodology used to arrive at the numbers and plots are shown in the appendix. 

We started by looking at some summary statistics and plotting of the response variable, birthweight. The sample mean of birthweight is 3378 grams and the sample variance is 259317. 
A histogram of birthweight is shown in Figure 1.1 below:

Birthweight appears to be unimodal, and slightly right skewed. There is a longer tail to the left, representing some outliers with very low birthweights. A boxplot of birthweight is show in Figure 1.2 below:

This further shows that there are a number of outliers, especially on the left of the distribution. These findings motivated the investigation as to whether removing these outliers could improve prediction accuracy. 

We examined a correlogram to look for pairwise correlations between the variables. A sample of correlogram data is shown in the heatmap (Figure 1.3):

In general, there was not strong pairwise correlation between the variables, except for between measurements of different PCBs. The individual variable with the highest correlation with the birthweight response variable was gestational period (rho = 0.543). Figure 1.4 shows a scatterplot of birthweight and gestational period, split by gender. 

Although gestational period is not a pollution-based predictor, the strength of correlation suggests that it would be prudent to include it or interactions with it in a model. 

Initial Data Analysis

As part of our exploratory data analysis, we fit an initial model with all covariates included, without any selection or optimization. Figure 1.5 shows a histogram of studentized residuals with a N(0,1) distribution overlaid. Figure 1.6 shows a Q-Q plot of the studentized residuals. 

These suggest normality under the model. Figure 1.7 shows a sample of some added-variable plots generated by partial regression.

The added-variable plots suggest linearity under the model. Figure 1.8 shows a plot of residuals against fitted values to assess heteroskedasticity.

The uniform distribution of residuals suggests that the observations have equal variance. Without the methodology of data collection, it is difficult to assess whether observations are independent, so we will assume independence for the purposes of this report. Subsequent models used for prediction will be tested for these assumptions in a similar fashion, however for brevity the plots will not be shown in the report. 


\pagebreak
## Methods

Our procedure is as follows:

1. Split the dataset into a training set and a test set. Here, our training set contains the first 600 observations and the test set contains the last 400. 
2. Fit Model 1 using this training set using the respective selection method.
3. Use DFFITS to pick out outliers, using the statistical rule of thumb $|DFFITS_i|>2\times\sqrt\frac{p+1}{n}$, where $p=$ number of covariates indicated by the best model fit and $n=600$.
4. Create a second training set excluding the outliers found above.
5. Fit Model 2 on the second training set using the same selection method in step 2.
6. Measure prediction accuracy with mean squared prediction error on the test set using Model 1.
7. Measure prediction accuracy with mean squared prediction error on the test set using Model 2.
8. Compare the two MSPEs.

We used the following selection methods: forward selection (no interactions and with interactions), backward selection (no interactions and interactions), manual selection and random selection (simulation). 

First, we fit the full model, then removed each covariate with the maximum VIF that satisfy VIF>10 each time we refit the model. This way, we were able to minimize multicollinearity among individual variates. Then, we used each of the mentioned selection methods to generate the best fitting model based on AIC, with data set being the training set. DFFITS are the scaled differences between the fitted value for $y_i$ and what we would have gotten if we hadn't observed $y_i$. A large value of DFFIT suggests the fitted values change substantially.

We chose DFFITS as our standard to pick out outliers because it incorporates information about both y-outliers and x-outliers. Furthermore, we do not need to refit the model for each $y_i$. Although both DFFITS and Cook's Distance can be generated using functions built in in R, DFFITS is more favorable over Cook's Distance because its statistical "rule of thumb" is more straightforward and yielded more reasonable outliers in our procedure.

For the full model in our selection methods, we considered both including the interactions and excluding the interactions to examine whether important interactions affect our results significantly.\

#### Simulation
During the process of selecting certain methods and comparing the models created before and after removing outliers, there are occasions in which we see an increase in MSPE after removing outliers and at other times we see a decrease. To verify which happens more often, we create a simulation to test many randomly selected models to identify a trend. We split the data into 4 sections to describe the effect of removing the outliers in each: chemicals, outdoors, lifestyles and others.
The simulation runs as follows: 
1.	Split the data into a training and a test set. The way the simulation is presented, the training sets that are used are [1:600], [101:700], [201:800], [301:900], [401:1000]. In each case, the other observations are used as the test case. This form of cross validation ensures that the results are not restricted to just removing the outliers in one of the sets.
2.	Generate a random set of 1s or 0s. Each one indicates that the column should be included in the model and the zero means not. We can also control the probability that a column is chosen or not. We generate as many of these sets of 1s and 0s as we want models in our sample. From these sets, we generate a list of vectors indicating which parameters to include in each model. ie. If the vector c(1,3,7) appears in the list, then one of the models in the sample will be the model created by including the 1st, 3rd, and 7th covariate as features (all models in the sample have the same dependent variable.) 
3.	For each of the models created by the sample above, we perform the method described previously, noting the DFFITS outliers and removing them to create a second model. We record the number of features, number of outliers, the MSPEs of the model against the current test set and the difference between the MSPEs (model with outliers â€“ model with outliers removed).

#### Results of the simulation. 
Running the simulation, we get a table of observations of different sets of covariates and the MSPE of both the normal model and the model with the outlier removed. If taking out the outliers from the training data set would improve the quality of the model, we would expect most of the observations to satisfy (mspe_original > mspe_no_outliers) in other words the difference between the original and the removed outlier version to be positive however that is not the case. The results give us a normal distribution. The sample mean of the 15000 observations gathered in the simulation is -3130.807 with a standard deviation of 5072.353. The test statistic that we observe in a hypothesis test that true mean is greater than 0 is -75.59489, low enough for R to describe that probability as 0.
However if we dissect the columns into the domains chemicals, outdoors, lifestyles and others, we notice that the chemicals and the miscellaneous domains contribute the most to the negative values while the other domains boast at least an expected decrease in MSPE after removing the outliers. In addition, combining the domains seems to combine the net expected outcome of removing the outliers.




### Analysis
Minimal Model: lm(e3_bw ~ 1, data = train.set)

Full Model: lm(e3_bw ~ ., data = train.set)\

Minimal model and full model is used in selection methods as initial and threshold model repectively.

\pagebreak
\textbf{Stepwise Selection}

We will do stepwise selection with the minimal and full models with training dataset. The new dataset will not include high influential points (outliers).
Below, there is a plot that highlights high influential points in red.
```{r, echo = FALSE, out.width="60%", fig.align = 'center'}
M0 <- lm(e3_bw ~ 1, data = train.set)
Mfull <- lm(e3_bw ~ ., data = train.set)
Mstep <- step(object = M0,
              scope = list(lower = M0, upper = Mfull), 
              direction = "both", trace = 0)
M <- Mstep
n <- nobs(M) ##600 observations
p <- length(M$coef)-1 ##21
dffits_m <- dffits(M) ##using DFFITS 


## plot DFFITS
plot(dffits_m,ylab = "DFFITS", main = "DFFITS plot of First model") 
abline(h=2*sqrt((p+1)/n),lty=2)  ## add thresholds
abline(h=-2*sqrt((p+1)/n),lty=2)
## highlight influential points
dff_ind <- which(abs(dffits_m)>2*sqrt((p+1)/n))
points(dffits_m[dff_ind]~dff_ind,col="red",pch=19) ## add red points
text(y=dffits_m[dff_ind],x=dff_ind, labels=dff_ind, pos=2) ## label high influence points

```

Now, we will do stepwise selection with the same minimal and full models but with new dataset. The new dataset will be the training dataset but it will not include high influential points (outliers).
Below, there is a plot with removed outliers.

```{r, echo = FALSE, out.width="60%", fig.align = 'center'}
#new dataset after removing outliers 
new.set <- train.set[-dff_ind,]

#fitting the same model with new dataset
M0 <- lm(e3_bw ~ 1, data = new.set)
Mfull <- lm(e3_bw ~ ., data = new.set)
Mstep.new <- step(object = M0,
              scope = list(lower = M0, upper = Mfull), 
              direction = "both", trace = 0)
M2 <- Mstep.new
n <- nobs(M2) ##600 observations
p <- length(M2$coef)-1 ##20
dffits_m <- dffits(M2)

## plot DFFITS with new data
plot(dffits_m,ylab = "DFFITS",  main = "DFFITS plot without outliers from First model") 
abline(h=2*sqrt((p+1)/n),lty=2)  ## add thresholds
abline(h=-2*sqrt((p+1)/n),lty=2)


M1.res <- test.set$e3_bw - # test observations
  predict(M, newdata = test.set) # prediction with training data
M2.res <- test.set$e3_bw - predict(M2, newdata = test.set)

```

```{r echo = FALSE, eval = FALSE}
#MSPE for First stepwise selected model
mean(M1.res^2)

#MSPE for no outliers stepwise selected model
mean(M2.res^2)

```

\pagebreak
## Results

\textbf{Stepwise Selection (no interactions)}

There were 38 high influential points for first model (where the dataset was training data).

1. MSPE for First stepwise selected model: 189595.1.
2. MSPE for no outliers stepwise selected model: 185221.9.

This shows that when the outliers were removed, the MSPE decreases. Also, we see that the data is more scattered in the second plot (after removing outliers). This is what we wanted.\

\textbf{Manual Selection (with interactions)}

1. Fit: Chemicals + others + interactions
    + MSPE for First model: 224859.6.
    + MSPE for no outlier model: 279121.2.
    
2. Fit: Outdoors + others + interactions
    + MSPE for First model: 189053.6.
    + MSPE for no outlier model: 188276.7.
    
3. Fit: Lifestyles + others + interactions
    + MSPE for First model: 199510.4.
    + MSPE for no outlier model:  198149.
    
4. Fit: Chemicals + interactions 
    + MSPE for First model: 317900.4.
    + MSPE for no outlier model: 335895.2.

For first and fourth fit, we see that the MSPE increases after outliers are removed. This is something unusual. This shows that the interactions between chemicals are not much related to each other and this makes sense.
However, for the second and third model, we see the expected decrease in  MSPE when outliers are removed.

\pagebreak
## Discussion
